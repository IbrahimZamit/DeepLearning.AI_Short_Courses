{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain: Models, Prompts and Output Parsers\n",
    "\n",
    "## Outline\n",
    "\n",
    " * Direct API calls to GroqAPI\n",
    " * API calls through LangChain:\n",
    "   * Prompts\n",
    "   * Models\n",
    "   * Output parsers\n",
    "\n",
    "Note: LLM's do not always produce the same results. When executing the code in your notebook, you may get slightly different answers that those in the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "client = Groq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    ")\n",
    "\n",
    "def get_completion(prompt, model):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=1 # 1 seems to give the best results\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The code below used to select the model version at the time of code execution. Nonetheless, because I am using the GroqAPI, I have much more flexibility in choosing the LLM to promt, and the one used in these notebooks is Meta's LLaMA3 70b model with a context window of 8,192 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# account for deprecation of LLM model\n",
    "import datetime\n",
    "# Get the current date\n",
    "current_date = datetime.datetime.now().date()\n",
    "\n",
    "# Define the date after which the model should be set to \"gpt-3.5-turbo\"\n",
    "target_date = datetime.date(2024, 6, 12)\n",
    "\n",
    "# Set the model variable based on the current date\n",
    "if current_date > target_date:\n",
    "    llm_model = \"gpt-3.5-turbo\"\n",
    "else:\n",
    "    llm_model = \"gpt-3.5-turbo-0301\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat API : GroqAI\n",
    "\n",
    "Let's start with a direct API calls to Groq."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"That's an easy one! 1 + 1 = 2.\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_completion(\"What is 1+1?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_email = \"\"\"\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse,\\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate the text that is delimited by triple backticks \n",
      "into a style that is American English in a calm and respectful tone\n",
      ".\n",
      "text: ```\n",
      "Arrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse,the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "style = \"\"\"American English \\\n",
    "in a calm and respectful tone\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"Translate the text \\\n",
    "that is delimited by triple backticks \n",
    "into a style that is {style}.\n",
    "text: ```{customer_email}```\n",
    "\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langchain is an AI model that can generate natural language prompts, and GroqAPI is a search engine that allows you to query large datasets using natural language queries. Combining the two can be a powerful way to explore and analyze large datasets.\n",
      "\n",
      "To use Langchain to prompt the GroqAPI, you can follow these general steps:\n",
      "\n",
      "1. **Generate a prompt with Langchain**: Use Langchain's API or a Langchain-powered interface (e.g., a chatbot or a web app) to generate a natural language prompt. For example, you can ask Langchain to generate a prompt related to a specific topic, like \"What are the top 10 most popular books about artificial intelligence?\"\n",
      "2. **Refine the prompt (optional)**: You can refine the generated prompt by modifying it to better suit your needs. This might involve adding or removing keywords, changing the tone, or adjusting the specificity of the question.\n",
      "3. **Send the prompt to the GroqAPI**: Take the refined prompt and send it to the GroqAPI as a query. You can do this by making an API request to the GroqAPI endpoint, passing the prompt as a parameter.\n",
      "4. **Process the GroqAPI response**: The GroqAPI will respond with a set of results that match your query. You can then process these results as needed, such as extracting specific information, visualizing the data, or using the results as input for further analysis.\n",
      "\n",
      "Here's an example of how you might use Langchain to generate a prompt and then send it to the GroqAPI:\n",
      "\n",
      "**Langchain-generated prompt**: \"What are the most popular AI-related research papers published in the last 5 years?\"\n",
      "\n",
      "**Refined prompt (optional)**: \"Show me the top 5 research papers about deep learning published in the last 2 years, sorted by citation count.\"\n",
      "\n",
      "**GroqAPI query**: Send the refined prompt to the GroqAPI endpoint using a tool like `curl` or a programming language like Python:\n",
      "```bash\n",
      "curl -X POST \\\n",
      "  https://api.groq.co/v1/query \\\n",
      "  -H 'Content-Type: application/json' \\\n",
      "  -d '{\"query\": \"Show me the top 5 research papers about deep learning published in the last 2 years, sorted by citation count.\"}'\n",
      "```\n",
      "**Process the GroqAPI response**: The GroqAPI will respond with a JSON object containing the top 5 research papers that match your query. You can then process this response as needed, such as extracting the paper titles, authors, and citation counts.\n",
      "\n",
      "Keep in mind that the specific implementation details will depend on your chosen programming language, the Langchain API or interface you're using, and the GroqAPI endpoint you're querying.\n"
     ]
    }
   ],
   "source": [
    "response = get_completion(\"How can I use Langchain to prompt the GroqAPI?\", model=\"llama3-70b-8192\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry for any confusion, but it seems there may be a misunderstanding. Langchain is a hypothetical concept of a blockchain-based platform for language processing, while GroqAPI is a specific API for querying data on the Groq database system.\n",
      "\n",
      "To use the GroqAPI, you should follow the documentation provided by Groq. Generally, you will need to:\n",
      "\n",
      "1. Send a query to the GroqAPI in the format specified by Groq.\n",
      "2. Handle the response, which will be in a format such as JSON or Protocol Buffers.\n",
      "\n",
      "Langchain, as a language processing platform, would not directly interact with an API like GroqAPI. Instead, Langchain might use data obtained via an API like GroqAPI for language processing tasks, such as natural language understanding or machine translation.\n",
      "\n",
      "If you want to use Langchain to process data from GroqAPI, you would first query the GroqAPI, obtain the desired data, then use Langchain to analyze the data and generate insights. However, as Langchain does not currently exist and is purely theoretical, it is not possible to provide a concrete example.\n"
     ]
    }
   ],
   "source": [
    "response = get_completion(\"How can I use Langchain to prompt the GroqAPI?\", model=\"mixtral-8x7b-32768\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat API : LangChain\n",
    "\n",
    "Let's try how we can do the same using LangChain. To use GroqAPi, we must install 'langchain-groq' instead of 'langchain', see [Official Docs](https://python.langchain.com/v0.2/docs/integrations/chat/groq/) for refrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-groq\n",
      "  Downloading langchain_groq-0.1.4-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: groq<1,>=0.4.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langchain-groq) (0.5.0)\n",
      "Requirement already satisfied: langchain-core<0.3,>=0.1.45 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langchain-groq) (0.2.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/codespace/.local/lib/python3.10/site-packages (from groq<1,>=0.4.1->langchain-groq) (4.3.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from groq<1,>=0.4.1->langchain-groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/codespace/.local/lib/python3.10/site-packages (from groq<1,>=0.4.1->langchain-groq) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from groq<1,>=0.4.1->langchain-groq) (2.7.1)\n",
      "Requirement already satisfied: sniffio in /home/codespace/.local/lib/python3.10/site-packages (from groq<1,>=0.4.1->langchain-groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /home/codespace/.local/lib/python3.10/site-packages (from groq<1,>=0.4.1->langchain-groq) (4.10.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/codespace/.local/lib/python3.10/site-packages (from langchain-core<0.3,>=0.1.45->langchain-groq) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langchain-core<0.3,>=0.1.45->langchain-groq) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langchain-core<0.3,>=0.1.45->langchain-groq) (0.1.62)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langchain-core<0.3,>=0.1.45->langchain-groq) (23.2)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/codespace/.local/lib/python3.10/site-packages (from langchain-core<0.3,>=0.1.45->langchain-groq) (8.2.3)\n",
      "Requirement already satisfied: idna>=2.8 in /home/codespace/.local/lib/python3.10/site-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain-groq) (3.6)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/codespace/.local/lib/python3.10/site-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain-groq) (1.2.0)\n",
      "Requirement already satisfied: certifi in /home/codespace/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /home/codespace/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/codespace/.local/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/codespace/.local/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1.45->langchain-groq) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.3,>=0.1.45->langchain-groq) (3.10.3)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.3,>=0.1.45->langchain-groq) (2.31.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (2.18.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core<0.3,>=0.1.45->langchain-groq) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core<0.3,>=0.1.45->langchain-groq) (2.0.7)\n",
      "Downloading langchain_groq-0.1.4-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: langchain-groq\n",
      "Successfully installed langchain-groq-0.1.4\n"
     ]
    }
   ],
   "source": [
    "#!pip install --upgrade langchain-groq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Low-latency Large Language Models (LLMs) are crucial in various applications where real-time or near-real-time processing is essential. Here are some reasons why low-latency LLMs are important:\\n\\n1. **Conversational AI**: In conversational AI, low-latency LLMs enable more natural and human-like interactions. Fast response times are critical in chatbots, virtual assistants, and voice assistants to maintain user engagement and provide a seamless experience.\\n2. **Real-time Language Translation**: Low-latency LLMs are vital in real-time language translation applications, such as simultaneous interpretation, live subtitles, and multilingual customer support. Fast processing ensures that translations are provided quickly, enabling effective communication across languages.\\n3. **Sentiment Analysis and Feedback**: In applications like customer feedback analysis, low-latency LLMs help process and analyze user feedback in real-time, enabling businesses to respond promptly to customer concerns and improve their services.\\n4. **Content Generation and Recommendation**: Low-latency LLMs can generate content, such as personalized product recommendations, in real-time, enhancing user experience and increasing engagement.\\n5. **Healthcare and Emergency Services**: In healthcare, low-latency LLMs can help analyze medical data, provide diagnosis, and suggest treatments quickly, which is critical in emergency situations. Similarly, in emergency services, fast language processing enables rapid response times, saving lives.\\n6. **Gaming and Interactive Systems**: In gaming and interactive systems, low-latency LLMs enable more realistic and responsive interactions, such as chatbots, NPCs (non-player characters), and voice-controlled interfaces.\\n7. **Edge AI and IoT**: With the proliferation of edge AI and IoT devices, low-latency LLMs are necessary to process and analyze data in real-time, enabling applications like smart homes, autonomous vehicles, and industrial automation.\\n8. **Accessibility and Inclusivity**: Low-latency LLMs can improve accessibility features, such as real-time transcription, translation, and text-to-speech synthesis, making technology more inclusive for people with disabilities.\\n9. **Cybersecurity and Threat Detection**: Fast language processing enables rapid detection and response to cyber threats, helping to prevent data breaches and protect sensitive information.\\n10. **Competitive Advantage**: In many industries, low-latency LLMs can provide a competitive advantage by enabling faster decision-making, improved customer experiences, and increased operational efficiency.\\n\\nIn summary, low-latency LLMs are essential in various applications where speed, responsiveness, and real-time processing are critical. They enable more natural human-machine interactions, improve decision-making, and provide a competitive edge in various industries.'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "chat = ChatGroq(temperature=0.0, model_name=\"llama3-70b-8192\")\n",
    "\n",
    "system = \"You are a helpful assistant.\"\n",
    "human = \"{text}\"\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n",
    "\n",
    "chain = prompt | chat\n",
    "chain.invoke({\"text\": \"Explain the importance of low latency LLMs.\"}).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x72fa96e1f0a0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x72fa96e1d690>, model_name='llama3-70b-8192', temperature=1e-08, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_string = \"\"\"Translate the text \\\n",
    "that is delimited by triple backticks \\\n",
    "into a style that is {style}. \\\n",
    "text: ```{text}```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af366c49-944d-4ad3-9bf9-cf0b5e386cc6",
   "metadata": {},
   "source": [
    "# Lesson 1: Why Pretraining?\n",
    "\n",
    "## 1. Install dependencies and fix seed\n",
    "\n",
    "Welcome to Lesson 1!\n",
    "\n",
    "If you would like to access the `requirements.txt` file for this course, go to `File` and click on `Open`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1773d6ea",
   "metadata": {},
   "source": [
    "## Update setuptools and wheel\n",
    "Ensure that you have the latest versions of setuptools and wheel, as they are needed for building packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57988922",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade setuptools wheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb03d167-6ebc-4da9-87c8-11ce1bd4eeaa",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "# Install any packages if it does not exist\n",
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3d6b90b-7f1f-4f1f-a5fb-47bea82d3896",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "# Ignore insignificant warnings (ex: deprecations)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b23a1a8-096f-4563-ab87-bbf3c2e699a2",
   "metadata": {
    "height": 183
   },
   "outputs": [],
   "source": [
    "# Set a seed for reproducibility\n",
    "import torch\n",
    "\n",
    "def fix_torch_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "fix_torch_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04c01c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Check for available GPU and set the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28600be",
   "metadata": {},
   "source": [
    "## 2. Download and save the model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f9b116",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Define the directory where you want to save the model and tokenizer\n",
    "directory = \"Pretraining LLMs/models/upstage\"\n",
    "\n",
    "# Load and cache the tokenizer and model in the specified directory\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"upstage/TinySolar-248m-4k\", cache_dir=directory)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"upstage/TinySolar-248m-4k\", cache_dir=directory)\n",
    "\n",
    "# Save the tokenizer and model to the specified directory\n",
    "tokenizer.save_pretrained(directory)\n",
    "model.save_pretrained(directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95414776-4bd9-4f87-93ce-2e4d1ec449b3",
   "metadata": {},
   "source": [
    "## 2. Load a general pretrained model\n",
    "\n",
    "This course will work with small models that fit within the memory of the learning platform. TinySolar-248m-4k is a small decoder-only model with 248M parameters (similar in scale to GPT2) and a 4096 token context window. You can find the model on the Hugging Face model library at [this link](https://huggingface.co/upstage/TinySolar-248m-4k).\n",
    "\n",
    "You'll load the model in three steps:\n",
    "1. Specify the path to the model in the Hugging Face model library\n",
    "2. Load the model using `AutoModelforCausalLM` in the `transformers` library\n",
    "3. Load the tokenizer for the model from the same model path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8405c72c-21d5-4a12-bbad-08a6e25be383",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "model_path_or_name = \"./models/upstage/TinySolar-248m-4k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cfe44f2-7376-4647-a0b3-719794316812",
   "metadata": {
    "height": 115
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 1024)\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (k_proj): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "          (up_proj): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "          (down_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# Check for available GPU and set the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "tiny_general_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path_or_name,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "# Move the model to the selected device\n",
    "tiny_general_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecf36736-ba12-4c73-bedb-1aaab9007220",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tiny_general_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path_or_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4d6924-b8a0-4e5b-a66d-e316e14a564d",
   "metadata": {},
   "source": [
    "## 3. Generate text samples\n",
    "\n",
    "Here you'll try generating some text with the model. You'll set a prompt, instantiate a text streamer, and then have the model complete the prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "804a5efc-45b1-4d93-8fde-df8a102d08e1",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "prompt = \"I am an AI engineer. I love\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "400eeca5-67ea-437c-a8fe-98d4a66d90e4",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "inputs = tiny_general_tokenizer(prompt, return_tensors=\"pt\")\n",
    "# Move the inputs to the same device as the model\n",
    "inputs = {key: value.to(device) for key, value in inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c1a9c24-5b1a-482c-a14a-90fc4334ceb1",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "streamer = TextStreamer(\n",
    "    tiny_general_tokenizer,\n",
    "    skip_prompt=True, # If you set to false, the model will first return the prompt and then the generated text\n",
    "    skip_special_tokens=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f41d5707-4ee1-4d82-a515-431235a3d775",
   "metadata": {
    "height": 166
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to write about the latest tech news, and this is my first time writing about it.\n",
      "I'm a big fan of the new \"AI\" and have been for a while. I've always loved the idea of being able to use the technology in ways that are not possible with traditional computers. I think that's why I like to write about things that are so important to me.\n",
      "The most recent example of this is the iPhone 5S. It was released back in 2013 and has been around since then. The phone is still very much a work in progress but it's now available\n"
     ]
    }
   ],
   "source": [
    "# Generate text\n",
    "with torch.no_grad():  # Ensure no gradients are calculated for generation\n",
    "    outputs = tiny_general_model.generate(\n",
    "        **inputs, \n",
    "        streamer=streamer, \n",
    "        use_cache=True,\n",
    "        max_new_tokens=128,\n",
    "        do_sample=False, \n",
    "        temperature=0.0,\n",
    "        repetition_penalty=1.1\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29640cf-3429-4e0c-be9c-a74833ddc29b",
   "metadata": {},
   "source": [
    "## 4. Generate Python samples with pretrained general model\n",
    "\n",
    "Use the model to write a python function called `find_max()` that finds the maximum value in a list of numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87d6a02-ec21-4352-8dec-203bf2cacf16",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "prompt =  \"def find_max(numbers):\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859db93d-c214-450b-8da3-57abfabf55b3",
   "metadata": {
    "height": 166
   },
   "outputs": [],
   "source": [
    "inputs = tiny_general_tokenizer(\n",
    "    prompt, return_tensors=\"pt\"\n",
    ").to(tiny_general_model.device)\n",
    "\n",
    "streamer = TextStreamer(\n",
    "    tiny_general_tokenizer, \n",
    "    skip_prompt=True, # Set to false to include the prompt in the output\n",
    "    skip_special_tokens=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de83f196-0118-4797-8894-c781a53c43a4",
   "metadata": {
    "height": 166
   },
   "outputs": [],
   "source": [
    "outputs = tiny_general_model.generate(\n",
    "    **inputs, \n",
    "    streamer=streamer, \n",
    "    use_cache=True, \n",
    "    max_new_tokens=128, \n",
    "    do_sample=False, \n",
    "    temperature=0.0, \n",
    "    repetition_penalty=1.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf5d693-9eb0-4394-8ad6-262ac5a48656",
   "metadata": {},
   "source": [
    "## 5. Generate Python samples with finetuned Python model\n",
    "\n",
    "This model has been fine-tuned on instruction code examples. You can find the model and information about the fine-tuning datasets on the Hugging Face model library at [this link](https://huggingface.co/upstage/TinySolar-248m-4k-code-instruct).\n",
    "\n",
    "You'll follow the same steps as above to load the model and use it to generate text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8669a4-384b-4de5-a4e3-7fecc81fd065",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "model_path_or_name = \"./models/upstage/TinySolar-248m-4k-code-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaf4752-c21d-44bd-8772-b1826d8f3bea",
   "metadata": {
    "height": 166
   },
   "outputs": [],
   "source": [
    "tiny_finetuned_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path_or_name,\n",
    "    device_map=\"cpu\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "tiny_finetuned_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path_or_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9b9233-c384-45ee-9fbb-cf39611b83bb",
   "metadata": {
    "height": 370
   },
   "outputs": [],
   "source": [
    "prompt =  \"def find_max(numbers):\"\n",
    "\n",
    "inputs = tiny_finetuned_tokenizer(\n",
    "    prompt, return_tensors=\"pt\"\n",
    ").to(tiny_finetuned_model.device)\n",
    "\n",
    "streamer = TextStreamer(\n",
    "    tiny_finetuned_tokenizer,\n",
    "    skip_prompt=True,\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "outputs = tiny_finetuned_model.generate(\n",
    "    **inputs,\n",
    "    streamer=streamer,\n",
    "    use_cache=True,\n",
    "    max_new_tokens=128,\n",
    "    do_sample=False,\n",
    "    temperature=0.0,\n",
    "    repetition_penalty=1.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee8f283-71da-4736-9ac1-ba4ce9d5367f",
   "metadata": {},
   "source": [
    "## 6. Generate Python samples with pretrained Python model\n",
    "\n",
    "Here you'll use a version of TinySolar-248m-4k that has been further pretrained (a process called **continued pretraining**) on a large selection of python code samples. You can find the model on Hugging Face at [this link](https://huggingface.co/upstage/TinySolar-248m-4k-py).\n",
    "\n",
    "You'll follow the same steps as above to load the model and use it to generate text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a3d3a3-1b57-4e56-98ad-58631485a58c",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "model_path_or_name = \"./models/upstage/TinySolar-248m-4k-py\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df7a061-ca41-41c3-8ffa-533a3f557d16",
   "metadata": {
    "height": 166
   },
   "outputs": [],
   "source": [
    "tiny_custom_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path_or_name,\n",
    "    device_map=\"cpu\",\n",
    "    torch_dtype=torch.bfloat16,    \n",
    ")\n",
    "\n",
    "tiny_custom_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path_or_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2240fd-19d6-4f44-89dc-9325f4fdc6b0",
   "metadata": {
    "height": 336
   },
   "outputs": [],
   "source": [
    "prompt = \"def find_max(numbers):\"\n",
    "\n",
    "inputs = tiny_custom_tokenizer(\n",
    "    prompt, return_tensors=\"pt\"\n",
    ").to(tiny_custom_model.device)\n",
    "\n",
    "streamer = TextStreamer(\n",
    "    tiny_custom_tokenizer,\n",
    "    skip_prompt=True, \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "outputs = tiny_custom_model.generate(\n",
    "    **inputs, streamer=streamer,\n",
    "    use_cache=True, \n",
    "    max_new_tokens=128, \n",
    "    do_sample=False, \n",
    "    repetition_penalty=1.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec94120",
   "metadata": {},
   "source": [
    "Try running the python code the model generated above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d109e788-2128-470d-8099-0a641938e062",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "def find_max(numbers):\n",
    "   max = 0\n",
    "   for num in numbers:\n",
    "       if num > max:\n",
    "           max = num\n",
    "   return max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868a767b-b5a1-4986-bef5-156a7e5a7acb",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "find_max([1,3,5,1,6,7,2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

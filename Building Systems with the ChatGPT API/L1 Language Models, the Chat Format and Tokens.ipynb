{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L1 Language Models, the Chat Format and Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "#### Load the API key and relevant Python libaries.\n",
    "In the course, a script is provided that loads an OpenAI API key for you. However, in my implementation, I'm customizing the code to use the GROQ API instead\n",
    "\n",
    "1. **Install Necessary Libraries**: First, make sure you have the `python-dotenv` library installed. If you haven't installed it yet, you can install it using pip:\n",
    "\n",
    "   ```bash\n",
    "   pip install python-dotenv\n",
    "2. **Create a `.env` File**: Create a file named `.env` in the same directory as your notebook. This file will store your API key. The `.env` file should follow the format: API_KEY=\"your_api_key_here\"\n",
    "\n",
    "3. **Load Environment Variables in Notebook**: In your Python notebook, you need to load the environment variables from the `.env` file. You can do this using the dotenv library:\n",
    "   ```bash\n",
    "   from dotenv import load_dotenv\n",
    "   import os\n",
    "   \n",
    "   # Load environment variables from .env file\n",
    "   load_dotenv()\n",
    "   \n",
    "   #Access the API key\n",
    "   #api_key = os.getenv(\"API_KEY\")\n",
    "\n",
    "4. **Add `.env` to .gitignore**: If you're using version control like Git, make sure to add the `.env` file to your .gitignore so that your API key isn't accidentally exposed or committed to your repository.\n",
    "\n",
    "That's it! Now your API key is stored securely as an environment variable in your Python notebook.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting groq\n",
      "  Downloading groq-0.5.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/codespace/.local/lib/python3.10/site-packages (from groq) (4.3.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/codespace/.local/lib/python3.10/site-packages (from groq) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from groq) (2.7.1)\n",
      "Requirement already satisfied: sniffio in /home/codespace/.local/lib/python3.10/site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /home/codespace/.local/lib/python3.10/site-packages (from groq) (4.10.0)\n",
      "Requirement already satisfied: idna>=2.8 in /home/codespace/.local/lib/python3.10/site-packages (from anyio<5,>=3.5.0->groq) (3.6)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/codespace/.local/lib/python3.10/site-packages (from anyio<5,>=3.5.0->groq) (1.2.0)\n",
      "Requirement already satisfied: certifi in /home/codespace/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->groq) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /home/codespace/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->groq) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/codespace/.local/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->groq) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->groq) (2.18.2)\n",
      "Downloading groq-0.5.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: groq\n",
      "Successfully installed groq-0.5.0\n",
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install groq\n",
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5.0\n"
     ]
    }
   ],
   "source": [
    "import groq\n",
    "# Checking the Groq version\n",
    "print(groq.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the API key\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "\n",
    "# the old-fashioned way!\n",
    "\n",
    "# Set the API key as an environment variable\n",
    "os.environ['API_KEY'] = 'your_key_here'\n",
    "api_key = os.environ.get('API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### helper function\n",
    "The function used may look familiar if you took the earlier \"ChatGPT Prompt Engineering for Developers\" Course. \n",
    "\n",
    "Throughout the course, they used OpenAI's `gpt-3.5-turbo` model and the [chat completions endpoint](https://platform.openai.com/docs/guides/chat).\n",
    "\n",
    "\n",
    "**Note**: In June 2023, OpenAI updated gpt-3.5-turbo. The results you see in the notebook may be slightly different than those in the video. Some of the prompts have also been slightly modified to produce the desired results.\n",
    "\n",
    "**Note**: Throughout this and all other lab notebooks documenting my implementation of this course, I will utilize the Groq library. To utilize the Groq API, please register for an account on [this page](https://console.groq.com/keys).  \n",
    "Below is the modified code we would use instead of the `get_completion` function. In this implementation, I will be using the Mistral AI `llama3-8b-8192` model, but feel free to experiment with different model outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "\n",
    "client = Groq(\n",
    "    api_key=api_key,\n",
    ")\n",
    "\n",
    "def get_completion(prompt, model=\"llama3-8b-8192\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=1 # 1 seems to give the best results\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt the model and get a completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_completion(\"What is the capital of Palestine?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Palestine is a matter of debate and controversy. The Palestine Liberation Organization (PLO), which is the national liberation movement fighting for the rights of the Palestinian people, claims that the capital of Palestine is Jerusalem, which is a major city in the Israeli-occupied West Bank.\n",
      "\n",
      "The Palestinian National Authority, which is the self-governing body established by the Oslo Accords, has claimed East Jerusalem as its intended capital.\n",
      "\n",
      "However, Israel asserts sovereignty over all of Jerusalem, including East Jerusalem, and considers it to be its undivided capital. The Israeli government has settled thousands of Jewish settlers in East Jerusalem, and has built museums, roads, and other infrastructure.\n",
      "\n",
      "In recent years, some countries, including Sweden, have recognized East Jerusalem as the capital of the State of Palestine. However, the majority of countries, including the United States, Israel, and most of the European Union member states, do not recognize Jerusalem as the capital of Palestine.\n",
      "\n",
      "In conclusion, the capital of Palestine is a disputed issue, and there is no universally recognized capital.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The letters in the word \"lollipop\" are:\n",
      "\n",
      "L-O-L-L-I-P-O-P\n",
      "\n",
      "Reversing these letters, I get:\n",
      "\n",
      "P-O-P-I-L-L-O-L\n"
     ]
    }
   ],
   "source": [
    "response = get_completion(\"Take the letters in lollipop and reverse them\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a stark contrat to the gpt-3.5-turbo model used in the course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_completion(\"\"\"Take the letters in l-o-l-l-i-p-o-p and reverse them\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The letters in l-o-l-l-i-p-o-p in reverse order are:\\n\\np-o-p-i-l-l-o-l'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function (chat format)\n",
    "Here's the helper function we'll use in this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion_from_messages(messages, \n",
    "                                 model=\"llama3-8b-8192\", \n",
    "                                 temperature=0, \n",
    "                                 max_tokens=500):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature, # this is the degree of randomness of the model's output\n",
    "        max_tokens=max_tokens, # the maximum number of tokens the model can ouptut \n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh my! Let me try,\n",
      "\n",
      "In the garden, oh so bright,\n",
      "A carrot grew with joy and light!\n",
      "Its orange hue was a wondrous sight,\n",
      "This happy carrot, dancing with delight!\n",
      "\n",
      "Its green top twirled with glee,\n",
      "As it soaked up the sunshine freely,\n",
      "No troubles plagued this happy spud,\n",
      "Just pure joy, with a carrot's good mood!\n"
     ]
    }
   ],
   "source": [
    "messages =  [  \n",
    "{'role':'system', \n",
    " 'content':\"\"\"You are an assistant who responds in the style of Dr Seuss.\"\"\"},    \n",
    "{'role':'user', \n",
    " 'content':\"\"\"write me a very short poem about a happy carrot\"\"\"},  \n",
    "] \n",
    "response = get_completion_from_messages(messages, temperature=1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_completion_and_token_count(messages, \n",
    "                                   model=\"llama3-8b-8192\", \n",
    "                                   temperature=1, \n",
    "                                   max_tokens=500):\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature, \n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "    \n",
    "    content = response.choices[0].message\n",
    "    \n",
    "    token_dict = {\n",
    "        'prompt_tokens': response.usage.prompt_tokens,\n",
    "        'completion_tokens':response.usage.completion_tokens,\n",
    "        'total_tokens':response.usage.total_tokens\n",
    "        }\n",
    "\n",
    "    return content, token_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "{'role':'system', \n",
    " 'content':\"\"\"You are an assistant who responds in the style of Dr Seuss.\"\"\"},    \n",
    "{'role':'user',\n",
    " 'content':\"\"\"write me a very short poem about a happy carrot\"\"\"},  \n",
    "] \n",
    "response, token_dict = get_completion_and_token_count(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, oh, the joyful sight!\n",
      "A happy carrot, glowing bright!\n",
      "Its orange hue, a vibrant flair,\n",
      "A crispy crunch, beyond compare!\n",
      "\n",
      "It beams with glee, a veggie delight,\n",
      "A bubbly burst of sunshine bright!\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt_tokens': 39, 'completion_tokens': 49, 'total_tokens': 88}\n"
     ]
    }
   ],
   "source": [
    "print(token_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A note about the backslash\n",
    "- In the course, we are using a backslash `\\` to make the text fit on the screen without inserting newline '\\n' characters.\n",
    "- GPT-3 isn't really affected whether you insert newline characters or not.  But when working with LLMs in general, you may consider whether newline characters in your prompt may affect the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
